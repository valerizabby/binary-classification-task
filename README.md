# Задача бинарной классификации для предсказания сердечного приступа

## Постановка задачи
Сравнить методы классификации на примере открытого датасета ![Personal Key Indicators of Heart Disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)

___

## Анализ данных
Датасет собран на основе опросов людей в США об их здоровье, бинарным признаком (вектором ответов) является стобец HeartDisease, который описывает, бывали ли у человека болезни сердца. Изначально датасет имел более 300 признаков, но был сжат до примерно 20 самых существенных, таких как курение, сахарный диабет, генетика etc. Автор предлагает его использовать в целях построения учебных моделей, так как датасет уже подчищен, единственный недостаток - несбалансированность. Так, для вектора ответов имеет место следующее распределение: 

![HeartDisease](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/%20HeartDisease.png)

Но это легко поправить с помощью OverSampling. Метод меньший класс раздувает до размеров большего. 

![Undersampling](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/overs.png)

Делим данные 7 к 3 на обучение и тест и начинаем обучение. 

## Методы классификации 

- **Метод опорных векторов (Support Vector Machines)**

SVM  - алгоритм из семейства линейных классификаторов, использующийся для задач классификации и регрессии. Цель SVM - найти уравнение разделяющей гиперплоскости, которая разделила бы классы оптимальным образом. 
Для понимания работы алгоритма, рассмотрим модель задачи бинарной классификации. Предположим, выборка данных линейно разделима. Это значит, что существует такое положение гиперплоскости, что любых объектов выборки положительны. Тогда нам интересно максимизировать ширину "полосы" между классами:

![SVM](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/svm.png)

- **Метод k-ближайших соседей (K-Nearest Neighbors)**
KNN - метрический классификатор, основанный на оценивании сходства близлежащих объектов. Классифицируемый объект относится к тому классу, к которому принадлежат k ближайших соседей. В случае бинарных классфикации, k берется нечетным, чтобы не было неопределеннсти. 

![KNN](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/knn.svg)

- **Случайный леc (Random Forest)**
RF — это множество решающих деревьев. В задаче классификации принимается решение голосованием по большинству. Решающее дерево представляет собой девовидную структуру, состоящую из решающих правил вида «Если ..., то ...». 
Правила автоматически генерируются в процессе обучения на обучающем множестве. Так как они формулируются практически на естественном языке, деревья решений как аналитические модели более интерпретируемы

![RF](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/RF.png)

## Обучение с помощью каждого метода, результаты
- Support Vector Machines
``` 
svclassifier = SVC(kernel = 'sigmoid')
svclassifier.fit(X_train, y_train)
```
- K-Nearest Neighbors
Модель:
```
KNN = KNeighborsClassifier(n_neighbors = 15)
KNN.fit(X_train, y_train)
```
Количество соседей выбрано экспериментальным путем, запустили модель с количеством соседей от 1 до 40 и посмотрим, где минимальная ошибка:

![KNNbest](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/knnbest.png)

- Random Forest
```
classifier_RF = RandomForestClassifier(n_estimators = 80, random_state = 0)
classifier_RF.fit(X_train, y_train)
```
тут параметр ``` n_estimators ``` отвечает за количество решающих деревьев в лесу. Очевидно, чем больше - тем лучше. Больше деревьев смогут проголосвать. Но увеличение параметра само собой увеличит время работы. 

## Сравнительная сводка 
Сравнивать работу методов будем с помощью roc-curve - характеристической кривой. В основе метода лежат понятия чувствительность (Se) и специфичность (Sp)
, где первое отражает долю истинно положительных ответов, а второе - долю истинно отрицательных. При построении кривой, по оси ординат откладывается Se, по оси абсцисс: 1 - Sp, то есть доля ложно положительных ответов. Таким образом, кривая имеет вид: 

![rocc](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/rocan.svg)

Рассмотрим roc-curve для сбалансированного и несбалансированного датасета. 
- Несбалансированный

![rocnet](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/roc_net.png)

- Oversampled

![rocover](https://github.com/valerizabby/binary-classification-task/blob/main/pictures/roc_over.png) 

